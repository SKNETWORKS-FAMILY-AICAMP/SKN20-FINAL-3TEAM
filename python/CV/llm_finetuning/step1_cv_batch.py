"""
Phase 1: CV ë°°ì¹˜ ì¶”ë¡  (ë¡œì»¬ GPU)
sampled_images.json ê¸°ë°˜ 2,000ì¥ í‰ë©´ë„ â†’ topology_graph.json ìƒì„±
"""

import json
import sys
import torch
from pathlib import Path
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from config import PipelineConfig
from training_utils import ProgressTracker

# ê¸°ì¡´ CV ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì„í¬íŠ¸
try:
    from CV.cv_inference.pipeline import InferencePipeline
    from CV.cv_inference.config import InferenceConfig
except ImportError as e:
    print(f"âŒ CV ì¶”ë¡  ëª¨ë“ˆì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print(f"   í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    sys.exit(1)


def run_cv_batch(config: PipelineConfig):
    """
    CV ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰

    Args:
        config: íŒŒì´í”„ë¼ì¸ ì„¤ì •
    """
    print("=" * 60)
    print("Phase 1: CV ë°°ì¹˜ ì¶”ë¡  (ë¡œì»¬ GPU)")
    print("=" * 60)

    # 1. GPU í™•ì¸
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"âœ… GPU: {gpu_name} ({gpu_memory:.1f} GB)")
    else:
        print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    # 2. ìƒ˜í”Œ ì´ë¯¸ì§€ ë¡œë“œ
    sampled_images_path = config.TRAINING_DATA_DIR / "sampled_images.json"

    if not sampled_images_path.exists():
        raise FileNotFoundError(
            f"sampled_images.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sampled_images_path}\n"
            f"Phase 0 (step0_sample_images.py)ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
        )

    sampled_stems = json.loads(sampled_images_path.read_text(encoding='utf-8'))
    print(f"âœ… ìƒ˜í”Œ ì´ë¯¸ì§€: {len(sampled_stems)}ê°œ")

    # 3. ì§„í–‰ë¥  ì¶”ì  ì´ˆê¸°í™”
    progress_file = config.PROGRESS_DIR / "cv_batch_progress.json"
    tracker = ProgressTracker(progress_file)

    remaining = tracker.get_remaining(sampled_stems)
    print(f"âœ… ì²˜ë¦¬ ëŒ€ê¸°: {len(remaining)}ê°œ")

    if not remaining:
        print("\nâœ… ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ!")
        return

    # 4. CV ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    print("\nğŸ”§ CV ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")

    # InferenceConfig ë¡œë“œ (ê¸°ì¡´ ì„¤ì • ì¬ì‚¬ìš©)
    inference_config = InferenceConfig()

    # InferencePipeline ì´ˆê¸°í™”
    try:
        pipeline = InferencePipeline(config=inference_config)
        pipeline.load_models()  # ëª¨ë¸ ë¡œë“œ í•„ìˆ˜
        print("âœ… CV ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

    # 5. ë°°ì¹˜ ì²˜ë¦¬
    print(f"\nğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ({len(remaining)}ê°œ)")
    print(f"   ì‹œê°í™” ì €ì¥: {config.SAVE_VISUALIZATION}")
    print(f"   CUDA ìºì‹œ ì •ë¦¬ ì£¼ê¸°: {config.CUDA_CACHE_CLEAR_INTERVAL}ê±´")

    success_count = 0
    failed_count = 0

    for idx, image_stem in enumerate(tqdm(remaining, desc="CV ì¶”ë¡ "), start=1):
        try:
            # ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„±
            image_path = config.IMAGE_DIR / f"{image_stem}.PNG"

            if not image_path.exists():
                # .png í™•ì¥ìë„ ì‹œë„
                image_path = config.IMAGE_DIR / f"{image_stem}.png"

            if not image_path.exists():
                print(f"\nâš ï¸  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_stem}")
                tracker.mark_failed(image_stem)
                failed_count += 1
                continue

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = config.OUTPUT_DIR / image_stem
            output_dir.mkdir(parents=True, exist_ok=True)

            # CV ì¶”ë¡  ì‹¤í–‰
            result = pipeline.run(
                image_path=image_path,
                save_json=False,  # ìš°ë¦¬ê°€ ì§ì ‘ ì €ì¥
                save_visualization=config.SAVE_VISUALIZATION
            )

            # topology_graph.json ì €ì¥
            topology_path = output_dir / "topology_graph.json"
            topology_path.write_text(
                json.dumps(result['topology_graph'], ensure_ascii=False, indent=2),
                encoding='utf-8'
            )

            # ì™„ë£Œ í‘œì‹œ
            tracker.mark_completed(image_stem)
            success_count += 1

        except Exception as e:
            print(f"\nâš ï¸  {image_stem} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            tracker.mark_failed(image_stem)
            failed_count += 1

        # CUDA ìºì‹œ ì •ë¦¬ (ì£¼ê¸°ì )
        if torch.cuda.is_available() and idx % config.CUDA_CACHE_CLEAR_INTERVAL == 0:
            torch.cuda.empty_cache()

    # 6. ê²°ê³¼ ìš”ì•½
    stats = tracker.get_stats()

    print("\n" + "=" * 60)
    print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
    print("=" * 60)
    print(f"ì„±ê³µ: {success_count}ê°œ")
    print(f"ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"ì´ ì™„ë£Œ: {stats['completed']}ê°œ")
    print(f"ì´ ì‹¤íŒ¨: {stats['failed']}ê°œ")

    if failed_count > 0:
        print(f"\nâš ï¸  ì‹¤íŒ¨í•œ ì´ë¯¸ì§€ ëª©ë¡: {tracker.failed}")

    print("=" * 60)


if __name__ == "__main__":
    config = PipelineConfig()
    run_cv_batch(config)
